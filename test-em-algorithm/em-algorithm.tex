\documentclass[9pt,draft,twocolumn]{jsarticle}
%\usepackage{txfonts}
\usepackage{times}
\usepackage{amsmath}
\usepackage{bm}
\usepackage[top=15truemm,bottom=20truemm,left=15truemm,right=15truemm]{geometry}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\sbm}{;\bm{m}}
\newcommand{\bmx}{{\bm{x}}}
\newcommand{\bmy}{{\bm{y}}}
\newcommand{\bmt}{{\bm\theta}}
\newcommand{\bmtb}{\bar\bmt}
\newcommand{\dsum}{\displaystyle\sum}
\begin{document}
\title{EMアルゴリズム考察}
\author{Shin Saito\\ \texttt{shinsa@jp.ibm.com}}
\maketitle
\begin{abstract}
この文書では、\emph{期待値最大化法}、通称\emph{EM法}または\emph{EMアルゴリズム (Expectation-Maximization Algorithm)}
についての理解を述べる。
\end{abstract}
\section{はじめに}
文献\cite{takamura10, kita99}を参考にした。

\section{パラメータ推定法}
確率モデルにおけるパラメータの推定法について述べる。
\subsection{最尤推定法}
$N$個の観測値の集まりであるサンプルデータ$D=\{x_1,
\ldots,x_N\}$が、\textbf{独立に、同一の確率分布に従って生成される(i.i.d.)}と仮定する。
また、観測値としては$d$次元ベクトルまたは$d$個の変数の並びを仮定する。
この確率分布が未知のパラメータを持つとした場合に、観測データから適切なパラメータを推定するのがパラメータ推定の問題である。

パラメータを$\theta$とすると、確率関数は$p_\theta(X)$や$p(X;\theta)$と書かれることが多い。ここでは後者を採用する。
すると独立性の仮定により、データ$D$の生起確率は
\[ P(D)= \prod_{x_i\in D} p(x_i;\theta) \]
となり、これは$\theta$の関数となる。これを\emph{尤度 (likelihood)}
といい、$L(\theta)$で表す。さらに、この尤度の対数を\emph{対数尤度 (log likelihood)} とよび、多くの場合$l(\theta)$で表す。
\[ l(\theta) = \log P(D)= \sum_{x_i\in D} \log p(x_i;\theta) \]
\emph{最尤推定法 (maximum likelihood estimation)} とは、対数尤度を最大化することによりパラメータを求める方法である。

\subsection{EM法}
EM法とは、パラメータ推定において、観測値の一部が得られない、つまり観測値が\textbf{不完全データ}である場合に、\emph{非観測変数
(unobserved variable)} または\emph{隠れ変数 (latent variable)}
の値とともにパラメータを推定する方法である。基本的にはパラメータに仮の値を割り当てて最尤推定を行い、
パラメータの値を更新していくと共に対数尤度を増加させる、という方法をとる。例を挙げる。
\subsubsection{例: 混合モデル}
隠れ変数は、内部状態を表す変数、と呼ばれることもある。そのネーミングによくマッチする例をあげる。

確率関数$p_i, \ldots,
p_M$から得られる混合モデル \[ P(x;\bm{\lambda})= \sum_{j=1}^M \lambda_j p_j(x) \quad
(\text{ただし、} \sum_j \lambda_j =1)
\]
に対して、観測データ$x_1,\ldots,x_N$からパラメータ$\bm{\lambda}$
$(=\lambda_1,\ldots,\lambda_M)$を推定する問題を考える。この混合モデルはまず確率$\lambda_j$で$j$を選び、$p_j$に従って$x_i$を生成する。
観測データからはどの$j$が選ばれたかわからないため、単純な最尤推定法は使えない。$x_i$を生成する際にどの$j$が選ばれたかを$q_i$で表すと、これが隠れ変数となる。 この場合、データ$x_1,\ldots,x_N$は不完全データであり、対応する完全データは$(x_1,q_1),\ldots,(x_N,q_N)$となる。
\section{クラスタリングとEM法}
そもそもEM法は様々な分野で用いられていたアルゴリズムの一般化として1977年に提案された。
ここで、EM法の直感的な理解への助けとして、クラスタリングに用いられる\emph{$k$-平均法 ($k$-means)}
を発展させることによってEM法との関連を見ていくことにする。

\subsection{クラスタリング}
クラスタリングとはおおまかに言うと、インスタンス
(観測値; observationともいう)
の集合$D=\{x_1,\ldots,x_N\}$を適当なクラスタ (部分集合) に分割することである。各インスタンスは$d$次元ベクトルのようなものを想定する。
クラスタリングの問題は、分割数があらかじめ指定されている場合とそうでない場合があるが、 今回は指定されているものとする。
さらに言うと、$D$を指定された数の集合に直和分割する問題を解くものとする。

\subsection{$k$-means}
$k$-meansは、与えられたインスタンス集合$D$を$k$個のクラスタ$C_1,\ldots,C_k$に直和分割するアルゴリズムであり、始めて公開されたのは1965年であると言われている。
クラスタリングは各クラスタ$C_j$の重心$m_j$がわかれば簡単に求まるがそれは未知である。
$k$-meansでは$m_j$に適当な初期値を代入することから始め、各インスタンスの属するクラスタと、各クラスタの重心を交互に更新することによって最適なクラスタリングを求める。
具体的なアルゴリズムは以下のようになる:
\begin{enumerate}
  \item $\forall j$. $m_j$に適当な初期値を代入し、以下の2と3を終了条件を満たすまで繰り返す:
  \item (Assignment Step) $\forall i$. $j^{(i)}= {\displaystyle\argmin_j}
  \ d(x_i,m_j)$ (ただし$d(x,m)$は$d$と$m$との距離)
  として、$x_i$をクラスタ$C_{j^{(i)}}$に加える。つまり、重心が最も近いクラスタに加える。このとき、クラスタへの割り当てが前回のループと全く変わらなかった場合、終了とする。
  \item (Update Step) $\forall j$.
  $m_j=\dfrac{\displaystyle\sum_{x_i \in C_j} x_i}{|C_j|}$
\end{enumerate}
(参考にした文献には明記されていないが) 
このアルゴリズムはいずれ終了し、ある種の意味で局所最適なクラスタリングを与える。局所最適であるので、初期値によって結果が異なる可能性がある。
あとで見るように、これは (ちょっと特殊な) 確率モデルの元での局所最尤推定になっていることがわかる。

\subsection{確率モデルとしてのクラスタリング}
前述のモデルでは各インスタンスは最も重心が近いクラスタに属するとした。これをより確率的な考え方にマッチするよう、モデルとその見方を変更する。

確率モデルとして捉えた場合、各インスタンスは、まずどのクラスタから生成されるかが (確率的に) 決められた後、そのクラスタから確率的に生成される、と考える。
つまり、インスタンス$x_i$がクラスタ$C_j$によって生成されるという事象の確率は
\[ p(x_i,j)= p(j) p(x_i | j) \]
となる。ここでの$p(x_i | j)$は混合モデルに現れた$p_j(x_i)$と同じものの表記であると思ってよい。さらにこれを全てのクラスタにわたって加えることにより、
$x_i$の (周辺確率としての) 生成確率が求まる。
\[ p(x_i)= \sum_j p(j) p(x_i | j) \]
つまりこれは、不完全データ$\{x_i\}$に対する完全データ (とパラメータ) $\{(x_i,j)\}$の推定問題であることがわかる。

\subsection{混合正規分布によるクラスタリング}
混合正規分布 (Gaussian mixture)では、クラスタからインスタンスを生成する確率分布として正規分布を仮定する。
つまり、クラスタの重心$m_j$およびインスタンスの偏差$\sigma$をパラメータとする正規分布$N(m_j, \sigma)$に従うとする。すると、インスタンスを$d$次元ベクトルであるとして、
\[ p(x_i | j; \bm{m}) = p(x_i | j; m_j) = \frac{1}{\sqrt{(2\pi\sigma^2)^d}} 
	\exp \left( -\frac{|x_i-m_j|^2}{2\sigma^2} \right)
\]
が成り立つ\footnote{$p(x_i | j; \bm{m})$とは$\bm{m}$をパラメータとする条件付き確率$p_{\bm{m}}(x_i | j)$のことである。
同様に、$p(x_i,j ; \bm{m})$とは$p_{\bm{m}}(x_i,j )$のことである。}。
ここでは単純化のために、$\sigma$を定数とし、クラスタによって変わらないとする%
\footnote{これを、クラスタごとに異なる、定数でないパラメータ$\sigma_j$である、としてもEM法で解くことができる。}。

前節のモデルをこの混合正規分布モデルを用いて拡張すると、$k$-meansにおけるループでの処理は以下のようになる。
\begin{enumerate}
  \item $\forall j$. $m_j$に適当な初期値を代入し、以下の2と3を終了条件を満たすまで繰り返す:
  \item (Assignment Step) $\forall i,j$.
  $x_i$がクラスタ$C_j$に属する事後確率$p(j|x_i)$を求める。
  前節のモデルではインスタンスはいずれか1つのクラスタに属したが、ここではそれぞれのクラスタに属している確からしさを求めることになる。
  \[
  \begin{split}
  p(j|x_i\sbm) &= \frac{ p(x_i,j\sbm) }{p(x_i\sbm) } = \dfrac{ p(x_i,j\sbm) }{ \displaystyle\sum_j
  p(x_i,j\sbm) } \\
  &= \dfrac{ p(j) p(x_i|j\sbm) }{ \displaystyle\sum_j p(j) p(x_i|j\sbm) }
  \end{split} 
  \]
  ここでクラスタの生成確率が一様であるとすると、$p(j)$を省くことができ、
  \[ p(j|x_i) 
  	= \dfrac{ p(x_i|j\sbm) }{ \displaystyle\sum_j p(x_i|j\sbm) }
  \]
  と計算できる形になる。なお、ループの終了条件については後で述べる。
  \item (Update Step) $\forall j$. ここでは前ステップで求めた事後確率を用いて、重み付きでクラスタの重心を計算し値を更新する。
  \[
  m_j=\dfrac{\displaystyle\sum_{x_i \in D} p(j|x_i\sbm) x_i}{\displaystyle\sum_{x_i \in D}
  p(j|x_i\sbm)}
  \]
\end{enumerate}
これをもとに考えると、前述の$k$-meansは混合正規分布におけるアルゴリズムを、ある確率分布に適用したものであることがわかる。つまり、$k$-meansは前述のアルゴリズムで、
$p(j|x_i)$を、$x_i$が$\bm{m}$の中で$m_j$に最も近いとき1となり、その他の場合に0となるように取ったものである。

また、これを一般化することにより、EM法のアルゴリズムが得られる。ここで述べたアルゴリズムの終了条件はEM法の終了条件を流用することができる。

\subsection{EM法}
ここでクラスタリングから一度離れて、パラメータ推定の一般的な問題を考える。
不完全な観測データ$\bm{x}_1,\ldots,\bm{x}_N$を確率モデル$p(\bm
x;\bm\theta)$から生成された観測データであるとし、 対応する完全データは$(\bm{x}_1,\bm{y}_1),\ldots,(\bm{x}_N,\bm{y}_N)$であるとする。
このときパラメータ$\bm{\theta}$を推定する問題を考える。

EM法は繰り返しにより、現在のパラメータ$\bm\theta$から対数尤度$l(\bm\theta)$を増加させるような新しいパラメータ$\bar{\bm\theta}$を求める方法である。
ここで、パラメータを更新した際の対数尤度の差を求めてみる。すると、
\[
\begin{split}
l(\bar{\bm\theta}) - l(\bm\theta) &= \sum_{\bm{x}_i} \log p(\bm x_i;\bmtb)
	- \sum_{\bm x_i} \log p(\bm x_i;\bmt) \\
	&= \text{(中略)} \\
	&\ge Q(\bmtb,\bmt) -Q(\bmt,\bmt)
\end{split}
\]
が成り立つ。ただし、
\[ Q(\bmtb,\bmt) = \sum_{\bm x_i} \sum_\bmy p(\bmy | \bmx_i ;\bmt) \log p(\bmx_i,\bmy;\bmtb) \]
である。この関数を$Q$関数とよぶ。$Q$関数を最大化するような$\bmtb$を求めることにより、対数尤度が増加していくことがわかる。
EM法の終了条件は、$Q$関数または対数尤度の増加が十分小さくなったとき、とすればよい。なお、EM法で求まるのは局所最適解であることに注意する。

なお、実際の計算の際には同時確率を展開して
\[
\begin{split}
Q(\bmtb,\bmt) &= \sum_{\bm x_i} \sum_\bmy p(\bmy | \bmx_i ;\bmt) 
	\log \left\{ p(\bmy;\bmtb) p(\bmx_i|\bmy;\bmtb) \right\} \\
&= \sum_{\bm x_i} \sum_\bmy p(\bmy | \bmx_i ;\bmt) 
	( \log p(\bmy;\bmtb) + \log p(\bmx_i|\bmy;\bmtb) )
\end{split} 
\]
とすることが多い。これは確率モデルの表現方法によって異なってくる。

\subsubsection{混合正規分布モデルとの関係}
前述の$Q$関数の式を、混合正規分布モデルでのアルゴリズムと比べてみると、$p(\bmy | \bmx_i
;\bmt)$の部分が、インスタンスが各クラスタに属する事後確率を求めている部分に相当することがわかる。
また、$Q$関数は$\log p(\bmx_i;\bmtb)$に関する期待値 (つまりデータの対数尤度) を求めていることがわかる。
これを最大化することにより、同じくある種の期待値である重心の更新式が得られるのは興味深いことである。

\subsubsection{(予想) 不完全データの一部がわかっている場合}
不完全データの一部がわかっている場合がある。例えば隠れ変数$y_1,y_2,y_3$のうち、いずれか1つだけは常にわかっている場合、などである。
参考文献には載っていなかったが、この場合、ある組み合せに対しては$\dsum_\bmy\cdots$が確実に計算でき、その場合は推定の精度がより高くなるか、
繰り返しの回数が減少することが予想される。

\bibliographystyle{junsrt}
\bibliography{em}
\end{document}